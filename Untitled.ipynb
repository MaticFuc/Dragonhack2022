{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c08704a-aa81-46f8-bb47-778ec05ea825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import teachify\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b595dc-3724-40d9-bdbf-9c1078ee93c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n1) Why did the Allies delay the invasion of France after Joseph Stalin requested it?\\na) They wanted to invade Sicily and Italy first.\\nb) They didn't have enough troops.\\nc) They were waiting for better weather conditions.\\nd) They wanted to stage a surprise attack.\\n\\n2) How many troops were needed for the invasion of France?\\na) 24,000\\nb) 50,000\\nc) 100,000\\nd) 200,000\\n\\n3) What was the main goal of the Normandy invasion?\\na) To free France from German occupation\\nb) To capture Caen\\nc) To destroy German forces\\nd) To secure a foothold in Europe\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teachify.form_query(prompt='MultipleChoice', article_name='Normandy_landings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc5f468-a6cd-47d5-8d15-e4110f057324",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"The Normandy landings were the landing operations and associated airborne operations on Tuesday, 6 June 1944 of the Allied invasion of Normandy in Operation Overlord during World War II. Codenamed Operation Neptune and often referred to as D-Day, it was the largest seaborne invasion in history. The operation began the liberation of France (and later western Europe) and laid the foundations of the Allied victory on the Western Front. \\\n",
    "Planning for the operation began in 1943. In the months leading up to the invasion, the Allies conducted a substantial military deception, codenamed Operation Bodyguard, to mislead the Germans as to the date and location of the main Allied landings. The weather on D-Day was far from ideal, and the operation had to be delayed 24 hours; a further postponement would have meant a delay of at least two weeks, as the invasion planners had requirements for the phase of the moon, the tides, and the time of day that meant only a few days each month were deemed suitable. Adolf Hitler placed Field Marshal Erwin Rommel in command of German forces and of developing fortifications along the Atlantic Wall in anticipation of an Allied invasion. U.S. President Franklin D. Roosevelt placed Major General Dwight D. Eisenhower in command of Allied forces. \\\n",
    "The amphibious landings were preceded by extensive aerial and naval bombardment and an airborne assault—the landing of 24,000 American, British, and Canadian airborne troops shortly after midnight. Allied infantry and armoured divisions began landing on the coast of France at 06:30. The target 50-mile (80 km) stretch of the Normandy coast was divided into five sectors: Utah, Omaha, Gold, Juno, and Sword. Strong winds blew the landing craft east of their intended positions, particularly at Utah and Omaha. The men landed under heavy fire from gun emplacements overlooking the beaches, and the shore was mined and covered with obstacles such as wooden stakes, metal tripods, and barbed wire, making the work of the beach-clearing teams difficult and dangerous. Casualties were heaviest at Omaha, with its high cliffs. At Gold, Juno, and Sword, several fortified towns were cleared in house-to-house fighting, and two major gun emplacements at Gold were disabled using specialised tanks.\\\n",
    "The Allies failed to achieve any of their goals on the first day. Carentan, Saint-Lô, and Bayeux remained in German hands, and Caen, a major objective, was not captured until 21 July. Only two of the beaches (Juno and Gold) were linked on the first day, and all five beachheads were not connected until 12 June; however, the operation gained a foothold that the Allies gradually expanded over the coming months. German casualties on D-Day have been estimated at 4,000 to 9,000 men. Allied casualties were documented for at least 10,000, with 4,414 confirmed dead. Museums, memorials, and war cemeteries in the area now host many visitors each year.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da5402e-6556-41b9-aa73-886783095d21",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multiple_choice_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate a multiple choice question and correct answer: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m article \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m =>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m res \u001b[38;5;241m=\u001b[39m teachify\u001b[38;5;241m.\u001b[39mperform_query(\u001b[43mmultiple_choice_query\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'multiple_choice_query' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"Generate a multiple choice question and correct answer: \\n\\n\" + article + \" =>\"\n",
    "res = teachify.perform_query(query)#[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a32ad56-4935-48b9-9fdb-fcaf3fd35189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-57l5PJpkf5K1Io6E9UQsZ70QYHs5y at 0x277b85a3cc0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nWhat was the codename for the Allied invasion of Normandy?\\n\\nA) Operation Neptune\\nB) Operation Bodyguard\\nC) D-Day\\nD) Operation Overlord\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1652530979,\n",
       "  \"id\": \"cmpl-57l5PJpkf5K1Io6E9UQsZ70QYHs5y\",\n",
       "  \"model\": \"text-davinci:002\",\n",
       "  \"object\": \"text_completion\"\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bd11e8-429a-4629-9c0f-de33db217702",
   "metadata": {},
   "source": [
    "## generate a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc470c7a-4664-4f01-b57b-70f8da7c4adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key =  \"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\" #jus\n",
    "\n",
    "def generate_questions_from_text(text, api_key, num_questions=1):\n",
    "    query = \"Generate a question from this text: \" + text + \" =>\"\n",
    "    #result = teachify.perform_query(query)[\"choices\"][0][\"text\"]\n",
    "        \n",
    "    openai.api_key = api_key\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt=query,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        n=num_questions\n",
    "        #stop=[\"\\n\"]\n",
    "    )\n",
    "\n",
    "    #print(response)\n",
    "    generated_questions = []\n",
    "    for i in range(num_questions): \n",
    "        result = response[\"choices\"][i][\"text\"]\n",
    "        generated_questions.append(result)\n",
    "        #print(result)\n",
    "        #splitted = result.split(\"\\n\\n\")\n",
    "        #question = splitted[1]\n",
    "        #answer = splitted[2]\n",
    "\n",
    "        #print(f\"generated question: {question}\")\n",
    "        #print(f\"generated answer: {answer}\")\n",
    "    return generated_questions\n",
    "\n",
    "generated_questions = generate_questions_from_text(article, api_key, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfa1627-1279-4cde-adf1-28dafdf5dd9a",
   "metadata": {},
   "source": [
    "## generate an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "973d22c0-e8e1-476c-bef5-5fcbe45e51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generated_question = \"What were the main goals of the Allies on D-Day?\" #result\n",
    "\n",
    "def generate_answer_to_the_question(question):\n",
    "\n",
    "    response_answer = openai.Answer.create(\n",
    "      search_model=\"ada\",\n",
    "      model=\"curie\",\n",
    "      question=question,\n",
    "      documents=[article],\n",
    "      examples_context=\"In 2017, U.S. life expectancy was 78.6 years.\",\n",
    "      examples=[[\"What is human life expectancy in the United States?\",\"Human life expectency is 78 years.\"]],\n",
    "      max_tokens=20,\n",
    "      stop=[\"\\n\", \"<|endoftext|>\"],\n",
    "    )\n",
    "    true_answer = response_answer[\"answers\"][0]\n",
    "    \n",
    "    return true_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3037e9c-64bb-4e16-bd94-2869ed55f4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main goals of the Allies on D-Day were to secure a foothold in France, to gain'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f050d-393a-4a6d-b094-389f049ae777",
   "metadata": {},
   "source": [
    "## generate false (random) answers, complete them from half of a correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a9c27d7-d6ee-4b7d-b0f4-cce6ff4e427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_false_answers(true_answer, num_false_answers):\n",
    "    # get half answers\n",
    "    half_answer = true_answer[:len(true_answer)//3]\n",
    "\n",
    "    query = \"Finish this sentence: \" +  half_answer +  \" => \"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt= query,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        n=num_false_answers\n",
    "        #stop=[\"\\n\"]\n",
    "    )\n",
    "\n",
    "\n",
    "    false_answers = []\n",
    "    for i in range(num_false_answers): \n",
    "        result = response[\"choices\"][i][\"text\"]\n",
    "        false_answers.append(result)\n",
    "\n",
    "    return false_answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0679fd13-f4c1-42e4-aaf2-113379e160aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'questions': [{'question': ' Why was D-Day significant in World War II?',\n",
       "   'true_answer': 'D-Day was significant in World War II because it was the first time that the Allies had landed',\n",
       "   'false_answers': ['\\n\\nD-Day was significant in World War II because it was the largest amphibious invasion in history.',\n",
       "    '\\n\\nD-Day was significant in World War II because it was the day that the Allies began their invasion of Normandy, France.',\n",
       "    ' D-Day was significant in World War Two because it was the day that the Allies invaded Normandy to liberate France from the Nazis.']},\n",
       "  {'question': '\\n\\nWhat was the codename for the Allied invasion of Normandy?',\n",
       "   'true_answer': 'Operation Overlord',\n",
       "   'false_answers': ['\\n\\nOperat ing', '\\n\\nOperate', '\\n\\nOperator']},\n",
       "  {'question': ' When was D-Day?',\n",
       "   'true_answer': 'D-Day was on 6 June 1944.',\n",
       "   'false_answers': ['\\n\\nD-Day was a turning point in World War II.',\n",
       "    '\\n\\nD-Day was the biggest military operation in history.',\n",
       "    '\\n\\nD-Day was a turning point in World War II.']}]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key =  \"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\" #jus\n",
    "\n",
    "def generate_MC_questions(text, num_questions, num_false_answers, api_key):\n",
    "\n",
    "    text_multiple_choice_qa_dict = {\"questions\": []}\n",
    "\n",
    "    questions = generate_questions_from_text(article, api_key, num_questions)\n",
    "\n",
    "    for q in questions:\n",
    "        question_dict = {\"question\":q}\n",
    "        #print(f\"generated question: {q}\")\n",
    "        answer = generate_answer_to_the_question(q)\n",
    "        question_dict[\"true_answer\"] = answer\n",
    "        #print(f\"generated true answer: {answer}\")\n",
    "        false_answers = generate_false_answers(answer, num_false_answers)\n",
    "        question_dict[\"false_answers\"] = false_answers\n",
    "        #print(f\"generated false answers: {false_answers}\")\n",
    "        text_multiple_choice_qa_dict[\"questions\"].append(question_dict)\n",
    "\n",
    "    return text_multiple_choice_qa_dict\n",
    "generate_MC_questions(article, 3, 3, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb1ea61-19e2-4bf3-b55a-43f07f184455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abac0d89-dc1a-43a8-b1f3-183791af0d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "result = res[\"choices\"][0][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a211a733-90f2-40fe-9c18-a918c071a30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GPT2Model' from 'transformers' (C:\\Users\\hladn\\miniconda3\\envs\\hashcode\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  GPT2Tokenizer, GPT2Model\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# add the EOS token as PAD token to avoid warnings\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'GPT2Model' from 'transformers' (C:\\Users\\hladn\\miniconda3\\envs\\hashcode\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import  GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2Model.from_pretrained(\"gpt2\",pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Load the BERT model. Various models trained on Natural Language Inference (NLI) https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md and \n",
    "# Semantic Textual Similarity are available https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md\n",
    "model_BERT = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4b3f19-1413-4551-bee7-c8e58fb8e96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configurationand file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'GPT2Model' from 'transformers' (C:\\Users\\hladn\\miniconda3\\envs\\hashcode\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2Model\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'GPT2Model' from 'transformers' (C:\\Users\\hladn\\miniconda3\\envs\\hashcode\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da9f689-e614-4389-b13a-f980ff0a3f15",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pipeline' from 'transformers' (C:\\Users\\hladn\\miniconda3\\envs\\hashcode\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, set_seed\n\u001b[0;32m      2\u001b[0m generator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m set_seed(\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pipeline' from 'transformers' (C:\\Users\\hladn\\miniconda3\\envs\\hashcode\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332a8e5-9201-46c8-a0dd-7eb56752c9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40a015-3da9-4493-b0e2-322941deae74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a1a54-927d-4f19-95f2-7f1e9f1ad012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "014b0cb6-7b82-4cb4-b6b3-b68b7e7ba96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hladn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hladn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hladn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc152c1-989f-45b1-9dbb-f828adb6cf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 6.344664573669434 secs.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'keyword_sentence_mapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m distractor_list\n\u001b[0;32m     68\u001b[0m key_distractor_list \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m \u001b[43mkeyword_sentence_mapping\u001b[49m:\n\u001b[0;32m     71\u001b[0m     wordsense \u001b[38;5;241m=\u001b[39m get_wordsense(keyword_sentence_mapping[keyword][\u001b[38;5;241m0\u001b[39m], keyword)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wordsense:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keyword_sentence_mapping' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "\n",
    "\n",
    "# Distractors from Wordnet\n",
    "def get_distractors_wordnet(syn, word):\n",
    "    distractors = []\n",
    "    word = word.lower()\n",
    "    orig_word = word\n",
    "    if len(word.split()) > 0:\n",
    "        word = word.replace(\" \", \"_\")\n",
    "    hypernym = syn.hypernyms()\n",
    "    if len(hypernym) == 0:\n",
    "        return distractors\n",
    "    for item in hypernym[0].hyponyms():\n",
    "        name = item.lemmas()[0].name()\n",
    "        # print (\"name \",name, \" word\",orig_word)\n",
    "        if name == orig_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\", \" \")\n",
    "        name = \" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in distractors:\n",
    "            distractors.append(name)\n",
    "    return distractors\n",
    "\n",
    "\n",
    "def get_wordsense(sent, word):\n",
    "    word = word.lower()\n",
    "\n",
    "    if len(word.split()) > 0:\n",
    "        word = word.replace(\" \", \"_\")\n",
    "\n",
    "    synsets = wn.synsets(word, 'n')\n",
    "    if synsets:\n",
    "        wup = max_similarity(sent, word, 'wup', pos='n')\n",
    "        adapted_lesk_output = adapted_lesk(sent, word, pos='n')\n",
    "        lowest_index = min(synsets.index(wup), synsets.index(adapted_lesk_output))\n",
    "        return synsets[lowest_index]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Distractors from http://conceptnet.io/\n",
    "def get_distractors_conceptnet(word):\n",
    "    word = word.lower()\n",
    "    original_word = word\n",
    "    if (len(word.split()) > 0):\n",
    "        word = word.replace(\" \", \"_\")\n",
    "    distractor_list = []\n",
    "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\" % (word, word)\n",
    "    obj = requests.get(url).json()\n",
    "\n",
    "    for edge in obj['edges']:\n",
    "        link = edge['end']['term']\n",
    "\n",
    "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\" % (link, link)\n",
    "        obj2 = requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2 = edge['start']['label']\n",
    "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
    "                distractor_list.append(word2)\n",
    "\n",
    "    return distractor_list\n",
    "\n",
    "\n",
    "key_distractor_list = {}\n",
    "\n",
    "for keyword in keyword_sentence_mapping:\n",
    "    wordsense = get_wordsense(keyword_sentence_mapping[keyword][0], keyword)\n",
    "    if wordsense:\n",
    "        distractors = get_distractors_wordnet(wordsense, keyword)\n",
    "        if len(distractors) == 0:\n",
    "            distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors\n",
    "    else:\n",
    "\n",
    "        distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors\n",
    "\n",
    "index = 1\n",
    "print(\"#############################################################################\")\n",
    "print(\n",
    "    \"NOTE::::::::  Since the algorithm might have errors along the way, wrong answer choices generated might not be correct for some questions. \")\n",
    "print(\"#############################################################################\\n\\n\")\n",
    "for each in key_distractor_list:\n",
    "    sentence = keyword_sentence_mapping[each][0]\n",
    "    pattern = re.compile(each, re.IGNORECASE)\n",
    "    output = pattern.sub(\" _______ \", sentence)\n",
    "    print(\"%s)\" % (index), output)\n",
    "    choices = [each.capitalize()] + key_distractor_list[each]\n",
    "    top4choices = choices[:4]\n",
    "    random.shuffle(top4choices)\n",
    "    optionchoices = ['a', 'b', 'c', 'd']\n",
    "    for idx, choice in enumerate(top4choices):\n",
    "        print(\"\\t\", optionchoices[idx], \")\", \" \", choice)\n",
    "    print(\"\\nMore options: \", choices[4:20], \"\\n\\n\")\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9cf5c-d88e-41bc-82c0-95f7e297d302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f4be5b-dbc2-4246-9adc-fabb20d47496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105b5ac-efa9-43a5-a3dc-d5d20bf52196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1306df-9f26-4a29-a67c-c51f3d563b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ceadc-f290-40a1-95a6-0f9183132abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0febb2c-7295-4c34-b4b2-a91d6c1a4543",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'summa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msumma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summarize\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbenepar\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'summa'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from summa.summarizer import summarize\n",
    "import benepar\n",
    "import string\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from random import shuffle\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "#this package is required for the summa summarizer\n",
    "nltk.download('punkt')\n",
    "benepar.download('benepar_en2')\n",
    "benepar_parser = benepar.Parser(\"benepar_en2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
